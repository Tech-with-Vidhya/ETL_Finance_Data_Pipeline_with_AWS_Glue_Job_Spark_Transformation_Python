# ETL_Finance_Data_Pipeline_with_AWS_Glue_Job_Spark_Transformation_Python
This project covers the implementation of building a ETL data pipeline using Python and AWS Services with a Spark transformation job for financial stocks trade transactions.  The Spark Transformation Job implemented using Python PySpark transforms the trade transactions data stored in the AWS S3 Bucket; to filter a sub-set of trade transactions for which the total number of shares transacted are less than or equal to 100.  Tools &amp; Technologies: Python, Boto3, PySpark, SDK, AWS CLI, AWS Virtual Private Cloud (VPC), AWS VPC Endpoint, AWS S3, AWS Glue, AWS Glue Crawler, AWS Glue Jobs, AWS Athena, AWS Redshift, Spark
